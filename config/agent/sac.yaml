name: sac
_target_: agent.sac.SACAgent

params:
  # filled at runtime by train_PEBBLE.py
  obs_dim: 0
  action_dim: 0
  action_range: [-1.0, 1.0]
  device: ${device}

  # --- SAC hparams (must match SACAgent.__init__) ---
  discount: 0.99
  init_temperature: 0.1

  alpha_lr: 0.0003
  alpha_betas: [0.9, 0.999]

  actor_lr: 0.0005
  actor_betas: [0.9, 0.999]
  actor_update_frequency: 1

  critic_lr: 0.0005
  critic_betas: [0.9, 0.999]
  critic_tau: 0.005
  critic_target_update_frequency: 2

  batch_size: 128
  learnable_temperature: true
  normalize_state_entropy: true

  # --- nested module configs (resolved inside params) ---
  critic_cfg:
    _target_: agent.critic.DoubleQCritic
    obs_dim: ${..obs_dim}
    action_dim: ${..action_dim}
    hidden_dim: 256
    hidden_depth: 2

  actor_cfg:
    _target_: agent.actor.DiagGaussianActor
    obs_dim: ${..obs_dim}
    action_dim: ${..action_dim}
    hidden_dim: 256
    hidden_depth: 2
    log_std_bounds: [-10, 2]
